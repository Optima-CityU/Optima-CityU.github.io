@article{e748a4d269e04bedb928d86bace27453,
 abstract = {Multi-task learning uses knowledge transfer among tasks to improve the generalization performance of all tasks. For deep multi-task learning, knowledge transfer is often implemented via sharing all hidden features of tasks. A major shortcoming is that it can lead to negative knowledge transfer across tasks when task correlation is weak. To overcome it, this paper proposes an evolutionary method to learn sparse sharing representations adaptively. By embedding the neural network optimization into evolutionary multitasking, our proposed method finds an optimal combination of tasks and sharing features. It can identify negative correlation and redundant features and then remove them from the hidden feature set. Thus, an optimal sparse sharing subnetwork can be produced for each task. Experiment results show that the proposed method achieve better learning performance with a smaller inference model than other related methods. Â© 2023 IEEE.},
 author = {Yayu Zhang and Yuhua Qian and Guoshuai Ma and Xinyan Liang and Guoqing Liu and Qingfu Zhang and Ke Tang},
 doi = {10.1109/TEVC.2023.3272663},
 issn = {1089-778X},
 journal = {IEEE Transactions on Evolutionary Computation},
 keywords = {Adaptation models, Correlation, evolutionary multitasking optimization, knowledge transfer, Multi-task learning, Multitasking, Optimization, sharing representation, Task analysis, Training, multitask learning (MTL)},
 language = {English},
 month = {June},
 number = {3},
 pages = {748--762},
 publisher = {Institute of Electrical and Electronics Engineers},
 title = {ESSR: Evolving Sparse Sharing Representation for Multi-task Learning},
 volume = {28},
 year = {2024}
}
