---
title: Discovering Interpretable Latent Space Directions for 3D-Aware Image Generation

# Authors
# A YAML list of author names
# If you created a profile for a user (e.g. the default `admin` user at `content/authors/admin/`), 
# write the username (folder name) here, and it will be replaced with their full name and linked to their profile.
authors:
- Zhiyuan Yang
- Qingfu Zhang

# Author notes (such as 'Equal Contribution')
# A YAML list of notes for each author in the above `authors` list
author_notes: []

date: '2024-06-01'

# Date to publish webpage (NOT necessarily Bibtex publication's date).
publishDate: '2024-10-30T07:28:57.863149Z'

# Publication type.
# A single CSL publication type but formatted as a YAML list (for Hugo requirements).
publication_types:
- article-journal

# Publication name and optional abbreviated publication name.
publication: '*IEEE Transactions on Emerging Topics in Computational Intelligence*'
publication_short: ''

doi: 10.1109/TETCI.2024.3369319

abstract: 2D GANs have yielded impressive results especially in image synthesis. However,
  they often encounter challenges with multi-view inconsistency due to the absence
  of 3D perception in their generation process. To overcome this shortcoming, 3D-aware
  GANs have been proposed to take advantage of both 3D representation methods, GANs,
  but it is very difficult to edit semantic attributes. To explore the semantic disentanglement
  in the 3D-aware latent space, this paper proposes a general framework, presents
  two representative approaches for the 3D manipulation task in both supervised, unsupervised
  manners. Our key idea is to utilize existing latent discovery methods, bring direct
  compatibility to 3D control. Specifically, we propose a novel module to extract
  the semantic latent space of the existing 3D-aware models, then develop two approaches
  to find a normal editing direction in the latent space. Leveraging the meaningful
  semantic latent directions, we can easily edit the shape, appearance attributes
  while preserving the 3D consistency. Quantitative, qualitative experiments show
  that our method is effective, efficient for the 3D-aware generation with steerability
  on both synthetic, real-world datasets. Â© 2024 IEEE.

# Summary. An optional shortened abstract.
summary: ''

tags:
- 3D-Aware image generation
- Aerospace electronics
- Generators
- Image synthesis
- implicit neural representations
- latent space discovery
- Rendering (computer graphics)
- Semantics
- Solid modeling
- Three-dimensional displays

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
---

Add the **full text** or **supplementary notes** for the publication here using Markdown formatting.
